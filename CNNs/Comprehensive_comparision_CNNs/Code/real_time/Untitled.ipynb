{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009bd127-09c6-4890-add3-8e07c30b2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00a86fd-f297-4d4e-8409-ed01955d5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vikrant/Documents/VT/Classes/deep-learning/Final-project/real_time\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7c2e1d2-b9e4-4bc5-8d1f-17d9a75d3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_prefix = \"/Users/vikrant/Documents/VT/Classes/deep-learning/Final-project/real_time/UCRArchive_2018/Adiac/Adiac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "328a2d0a-eb18-4fef-a1ad-8a1c2b05d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "\n",
    "train_data_with_labels = np.genfromtxt(file_name_prefix+'_TRAIN.tsv')\n",
    "test_data_with_labels = np.genfromtxt(file_name_prefix+'_TEST.tsv')\n",
    "\n",
    "data_with_labels = np.vstack( (train_data_with_labels, test_data_with_labels))\n",
    "data = data_with_labels[:,1:]\n",
    "labels = data_with_labels[:,0]\n",
    "\n",
    "\n",
    "# We make sure that labels are numbered as 0, 1, 2, ... \n",
    "# and set the number of classes\n",
    "\n",
    "min_label = min(labels)\n",
    "max_label = max(labels)\n",
    "if min_label == 0:\n",
    "  NUM_CLASSES = int(max_label+1)\n",
    "elif min_label == 1:\n",
    "  labels = labels - min_label\n",
    "  NUM_CLASSES = int(max_label)\n",
    "elif min_label == -1:\n",
    "  if np.sum(labels == -1)+np.sum(labels==1) == len(labels):\n",
    "    NUM_CLASSES = 2\n",
    "    labels[labels==-1]=0\n",
    "  else:\n",
    "    raise Exception(\"Unexpected labels\")\n",
    "else:\n",
    "  raise Exception(\"Unexpected labels\")\n",
    "\n",
    "# We make sure that the length of the time series is a multiple of 4 \n",
    "# (for Net1, we acutally only need the length to be a multiple of 2,\n",
    "# but we need the length to be a multiple of 4 for Net2)\n",
    "\n",
    "NUM_INPUT_FEATURES = len(data[0]) \n",
    "values_to_cut = NUM_INPUT_FEATURES % 4\n",
    "if values_to_cut != 0:\n",
    "  data = data[:,0:NUM_INPUT_FEATURES-values_to_cut]\n",
    "  NUM_INPUT_FEATURES = NUM_INPUT_FEATURES - values_to_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b426b6ca-3a98-45c5-b235-b84f284e88cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython\n",
      "  Downloading Cython-3.0.12-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m738.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cython\n",
      "Successfully installed cython-3.0.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6fb6715-4500-4ad3-bbcb-fab8746df624",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c88cf37e-c7f0-48cd-aa3e-19ea36b95082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of stderr:\n",
      "In file included from /Users/vikrant/.ipython/cython/_cython_magic_9c771bd9c84b20df6b876af06aa8362c247b22b8.c:1250:\n",
      "In file included from /opt/anaconda3/lib/python3.12/site-packages/numpy/core/include/numpy/arrayobject.h:5:\n",
      "In file included from /opt/anaconda3/lib/python3.12/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:\n",
      "In file included from /opt/anaconda3/lib/python3.12/site-packages/numpy/core/include/numpy/ndarraytypes.h:1929:\n",
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: \"Using deprecated NumPy API, disable it with \"          \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "   17 | #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "      |  ^\n",
      "1 warning generated.\n",
      "ld: warning: duplicate -rpath '/opt/anaconda3/lib' ignored"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "def dtw(np.ndarray[np.float_t, ndim=1] ts1, \n",
    "        np.ndarray[np.float_t, ndim=1] ts2):\n",
    "  \n",
    "  cdef int LEN_TS1 \n",
    "  cdef int LEN_TS2\n",
    "  cdef int i\n",
    "  cdef int j\n",
    "  cdef np.ndarray[np.float_t, ndim=2] dtw_matrix\n",
    "\n",
    "  LEN_TS1 = len(ts1)\n",
    "  LEN_TS2 = len(ts2)\n",
    "\n",
    "  dtw_matrix = np.zeros( (LEN_TS1, LEN_TS2), dtype=np.float )\n",
    "  \n",
    "  dtw_matrix[0,0] = abs(ts1[0]-ts2[0])\n",
    "  \n",
    "  for i in range(1, LEN_TS1):\n",
    "    dtw_matrix[i,0] = dtw_matrix[i-1,0]+abs(ts1[i]-ts2[0])\n",
    "\n",
    "  for j in range(1, LEN_TS2):\n",
    "    dtw_matrix[0,j] = dtw_matrix[0,j-1]+abs(ts1[0]-ts2[j])\n",
    "\n",
    "  for i in range(1, LEN_TS1):\n",
    "    for j in range(1, LEN_TS2):\n",
    "      dtw_matrix[i,j] = min(dtw_matrix[i-1,j-1], dtw_matrix[i-1,j], \n",
    "                            dtw_matrix[i, j-1]) + abs(ts1[i]-ts2[j])\n",
    "  \n",
    "  return dtw_matrix[ len(ts1)-1, len(ts2)-1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26d08c73-d533-45b8-8ed0-89185c85608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_activations(data, convolutional_filters):\n",
    "  \"\"\"\n",
    "  Calculation of the activations of the dynamic convolutional layer.\n",
    "\n",
    "  Inputs\n",
    "  ------\n",
    "    data : np.array \n",
    "      Two-dimensional array containing the input data, \n",
    "      each row of the array corresponds to one of the time series\n",
    "    convolutional_filters : np.array\n",
    "      Three-dimensional array containing the parameters of the dynamic \n",
    "      convolutional layer. The first index corresponds to the output channel\n",
    "      of the convolution, the second index corresponds to the input channel \n",
    "      (the current implementation only works with 1 input channel, so the second\n",
    "      index is always zero), the third index is the position within the local\n",
    "      pattern corresponding to a convolutional filter\n",
    "  \"\"\"\n",
    "  num_instances = len(data)\n",
    "  length_of_time_series = len(data[0])\n",
    "  num_conv_filters = len(convolutional_filters)\n",
    "  conv_filter_size = len(convolutional_filters[0][0])\n",
    "\n",
    "  activations = np.zeros( (num_instances, num_conv_filters, \n",
    "                           length_of_time_series-conv_filter_size+1) )\n",
    "  for i in range(num_instances):\n",
    "    for j in range(length_of_time_series-conv_filter_size+1):\n",
    "      for k in range(num_conv_filters):\n",
    "        activations[i,k,j] = dtw(convolutional_filters[k][0],\n",
    "                                 data[i,j:j+conv_filter_size])\n",
    "        \n",
    "  return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8289354-c83d-4dfa-bd3d-78c09f4c83cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_FILTERS = 25\n",
    "CONV_FILTERS2 = 10\n",
    "CONV_FILTER_SIZE = 9\n",
    "\n",
    "class Net1_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1_CNN, self).__init__()\n",
    "        num_units_fc = 100\n",
    "        self.num_inputs_fc = int(CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = CONV_FILTERS, \n",
    "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(self.num_inputs_fc, num_units_fc)\n",
    "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, NUM_INPUT_FEATURES)\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1, self.num_inputs_fc)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net2_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2_CNN, self).__init__()\n",
    "        num_units_fc = 100\n",
    "        self.num_inputs_fc = int(CONV_FILTERS2*((NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2-CONV_FILTER_SIZE+1)/2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = CONV_FILTERS, \n",
    "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels = CONV_FILTERS, out_channels = CONV_FILTERS2, \n",
    "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
    "        self.max_pool2 = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(self.num_inputs_fc, num_units_fc)\n",
    "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, NUM_INPUT_FEATURES)\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = x.view(-1, self.num_inputs_fc)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Please note that the dynamic convolutional layer is initialized using the \n",
    "# parameters learned during the \"pre-train\" phase (in which a \"usual\" \n",
    "# convolutional network is trained). Once the pre-train phased is completed, \n",
    "# the parameters of the dynamic convoltuional layer are fixed, therefore,\n",
    "# the activations of the dynamic convolutional layer will be pre-calculated \n",
    "# outside the network for efficient implementation.\n",
    "\n",
    "class Net1_DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1_DCNN, self).__init__()\n",
    "        num_units_fc = 100\n",
    "\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(int(CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2), num_units_fc)\n",
    "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, CONV_FILTERS, NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1,int(CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2))\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net2_DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2_DCNN, self).__init__()\n",
    "        num_units_fc = 100\n",
    "        self.num_inputs_fc = int(CONV_FILTERS2*((NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2-CONV_FILTER_SIZE+1)/2)\n",
    "\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels = CONV_FILTERS, out_channels = CONV_FILTERS2, \n",
    "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
    "        self.max_pool2 = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(self.num_inputs_fc, num_units_fc)\n",
    "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, CONV_FILTERS, NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = x.view(-1, self.num_inputs_fc)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e98aa24-f091-43da-853a-e259f33636c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, test_data, device):\n",
    "  test_dataset = torch.utils.data.TensorDataset( \n",
    "    torch.Tensor(test_data), \n",
    "    torch.LongTensor(test_labels)\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(test_dataset)\n",
    "\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for inputs, targets in testloader:\n",
    "      inputs = inputs.to(device) \n",
    "      targets = targets.to(device) \n",
    "      outputs = net(inputs)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      total += targets.size(0)\n",
    "      correct += (predicted == targets)\n",
    "  \n",
    "  return float(correct/total), float(correct), float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eec1f6b7-eba6-4ec3-8966-18f4fa68faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, we will use CPU instead of GPU\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.614\n",
      "epoch: 100 loss: 3.449\n",
      "epoch: 200 loss: 2.878\n",
      "epoch: 300 loss: 2.427\n",
      "epoch: 400 loss: 2.143\n",
      "epoch: 500 loss: 1.949\n",
      "epoch: 600 loss: 1.799\n",
      "epoch: 700 loss: 1.674\n",
      "epoch: 800 loss: 1.563\n",
      "epoch: 900 loss: 1.461\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 3.945\n",
      "epoch: 100 loss: 3.261\n",
      "epoch: 200 loss: 2.706\n",
      "epoch: 300 loss: 2.311\n",
      "epoch: 400 loss: 2.028\n",
      "epoch: 500 loss: 1.827\n",
      "epoch: 600 loss: 1.674\n",
      "epoch: 700 loss: 1.551\n",
      "epoch: 800 loss: 1.444\n",
      "epoch: 900 loss: 1.349\n",
      "Fold:  1, accuracy of CNN:  0.506\n",
      "Fold:  1, accuracy of DCNN: 0.620\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.615\n",
      "epoch: 100 loss: 3.447\n",
      "epoch: 200 loss: 2.905\n",
      "epoch: 300 loss: 2.457\n",
      "epoch: 400 loss: 2.115\n",
      "epoch: 500 loss: 1.895\n",
      "epoch: 600 loss: 1.742\n",
      "epoch: 700 loss: 1.624\n",
      "epoch: 800 loss: 1.529\n",
      "epoch: 900 loss: 1.447\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 4.073\n",
      "epoch: 100 loss: 3.228\n",
      "epoch: 200 loss: 2.487\n",
      "epoch: 300 loss: 2.021\n",
      "epoch: 400 loss: 1.743\n",
      "epoch: 500 loss: 1.552\n",
      "epoch: 600 loss: 1.404\n",
      "epoch: 700 loss: 1.283\n",
      "epoch: 800 loss: 1.180\n",
      "epoch: 900 loss: 1.093\n",
      "Fold:  2, accuracy of CNN:  0.564\n",
      "Fold:  2, accuracy of DCNN: 0.692\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.619\n",
      "epoch: 100 loss: 3.433\n",
      "epoch: 200 loss: 2.861\n",
      "epoch: 300 loss: 2.410\n",
      "epoch: 400 loss: 2.137\n",
      "epoch: 500 loss: 1.951\n",
      "epoch: 600 loss: 1.802\n",
      "epoch: 700 loss: 1.678\n",
      "epoch: 800 loss: 1.571\n",
      "epoch: 900 loss: 1.475\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 4.023\n",
      "epoch: 100 loss: 3.165\n",
      "epoch: 200 loss: 2.511\n",
      "epoch: 300 loss: 2.100\n",
      "epoch: 400 loss: 1.836\n",
      "epoch: 500 loss: 1.649\n",
      "epoch: 600 loss: 1.506\n",
      "epoch: 700 loss: 1.387\n",
      "epoch: 800 loss: 1.286\n",
      "epoch: 900 loss: 1.199\n",
      "Fold:  3, accuracy of CNN:  0.526\n",
      "Fold:  3, accuracy of DCNN: 0.641\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.613\n",
      "epoch: 100 loss: 3.393\n",
      "epoch: 200 loss: 2.862\n",
      "epoch: 300 loss: 2.454\n",
      "epoch: 400 loss: 2.167\n",
      "epoch: 500 loss: 1.964\n",
      "epoch: 600 loss: 1.805\n",
      "epoch: 700 loss: 1.678\n",
      "epoch: 800 loss: 1.568\n",
      "epoch: 900 loss: 1.472\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 3.933\n",
      "epoch: 100 loss: 3.153\n",
      "epoch: 200 loss: 2.333\n",
      "epoch: 300 loss: 1.893\n",
      "epoch: 400 loss: 1.634\n",
      "epoch: 500 loss: 1.450\n",
      "epoch: 600 loss: 1.304\n",
      "epoch: 700 loss: 1.184\n",
      "epoch: 800 loss: 1.081\n",
      "epoch: 900 loss: 0.996\n",
      "Fold:  4, accuracy of CNN:  0.462\n",
      "Fold:  4, accuracy of DCNN: 0.551\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.622\n",
      "epoch: 100 loss: 3.452\n",
      "epoch: 200 loss: 2.858\n",
      "epoch: 300 loss: 2.374\n",
      "epoch: 400 loss: 2.086\n",
      "epoch: 500 loss: 1.886\n",
      "epoch: 600 loss: 1.732\n",
      "epoch: 700 loss: 1.607\n",
      "epoch: 800 loss: 1.497\n",
      "epoch: 900 loss: 1.398\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 3.895\n",
      "epoch: 100 loss: 3.192\n",
      "epoch: 200 loss: 2.501\n",
      "epoch: 300 loss: 2.108\n",
      "epoch: 400 loss: 1.865\n",
      "epoch: 500 loss: 1.689\n",
      "epoch: 600 loss: 1.552\n",
      "epoch: 700 loss: 1.433\n",
      "epoch: 800 loss: 1.329\n",
      "epoch: 900 loss: 1.237\n",
      "Fold:  5, accuracy of CNN:  0.654\n",
      "Fold:  5, accuracy of DCNN: 0.667\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.615\n",
      "epoch: 100 loss: 3.425\n",
      "epoch: 200 loss: 2.814\n",
      "epoch: 300 loss: 2.414\n",
      "epoch: 400 loss: 2.146\n",
      "epoch: 500 loss: 1.959\n",
      "epoch: 600 loss: 1.820\n",
      "epoch: 700 loss: 1.707\n",
      "epoch: 800 loss: 1.611\n",
      "epoch: 900 loss: 1.526\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 4.232\n",
      "epoch: 100 loss: 3.215\n",
      "epoch: 200 loss: 2.543\n",
      "epoch: 300 loss: 2.130\n",
      "epoch: 400 loss: 1.876\n",
      "epoch: 500 loss: 1.700\n",
      "epoch: 600 loss: 1.563\n",
      "epoch: 700 loss: 1.450\n",
      "epoch: 800 loss: 1.355\n",
      "epoch: 900 loss: 1.271\n",
      "Fold:  6, accuracy of CNN:  0.590\n",
      "Fold:  6, accuracy of DCNN: 0.641\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.615\n",
      "epoch: 100 loss: 3.405\n",
      "epoch: 200 loss: 2.884\n",
      "epoch: 300 loss: 2.480\n",
      "epoch: 400 loss: 2.164\n",
      "epoch: 500 loss: 1.933\n",
      "epoch: 600 loss: 1.759\n",
      "epoch: 700 loss: 1.619\n",
      "epoch: 800 loss: 1.502\n",
      "epoch: 900 loss: 1.398\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 4.061\n",
      "epoch: 100 loss: 3.232\n",
      "epoch: 200 loss: 2.611\n",
      "epoch: 300 loss: 2.184\n",
      "epoch: 400 loss: 1.896\n",
      "epoch: 500 loss: 1.697\n",
      "epoch: 600 loss: 1.546\n",
      "epoch: 700 loss: 1.418\n",
      "epoch: 800 loss: 1.307\n",
      "epoch: 900 loss: 1.209\n",
      "Fold:  7, accuracy of CNN:  0.590\n",
      "Fold:  7, accuracy of DCNN: 0.603\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.614\n",
      "epoch: 100 loss: 3.406\n",
      "epoch: 200 loss: 2.834\n",
      "epoch: 300 loss: 2.405\n",
      "epoch: 400 loss: 2.144\n",
      "epoch: 500 loss: 1.950\n",
      "epoch: 600 loss: 1.796\n",
      "epoch: 700 loss: 1.666\n",
      "epoch: 800 loss: 1.554\n",
      "epoch: 900 loss: 1.457\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 4.179\n",
      "epoch: 100 loss: 3.176\n",
      "epoch: 200 loss: 2.493\n",
      "epoch: 300 loss: 2.084\n",
      "epoch: 400 loss: 1.845\n",
      "epoch: 500 loss: 1.677\n",
      "epoch: 600 loss: 1.543\n",
      "epoch: 700 loss: 1.429\n",
      "epoch: 800 loss: 1.329\n",
      "epoch: 900 loss: 1.237\n",
      "Fold:  8, accuracy of CNN:  0.551\n",
      "Fold:  8, accuracy of DCNN: 0.615\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.613\n",
      "epoch: 100 loss: 3.392\n",
      "epoch: 200 loss: 2.820\n",
      "epoch: 300 loss: 2.395\n",
      "epoch: 400 loss: 2.136\n",
      "epoch: 500 loss: 1.966\n",
      "epoch: 600 loss: 1.840\n",
      "epoch: 700 loss: 1.737\n",
      "epoch: 800 loss: 1.645\n",
      "epoch: 900 loss: 1.560\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 3.901\n",
      "epoch: 100 loss: 3.160\n",
      "epoch: 200 loss: 2.380\n",
      "epoch: 300 loss: 1.940\n",
      "epoch: 400 loss: 1.685\n",
      "epoch: 500 loss: 1.500\n",
      "epoch: 600 loss: 1.356\n",
      "epoch: 700 loss: 1.238\n",
      "epoch: 800 loss: 1.140\n",
      "epoch: 900 loss: 1.055\n",
      "Fold:  9, accuracy of CNN:  0.615\n",
      "Fold:  9, accuracy of DCNN: 0.782\n",
      "Training CNN...\n",
      "epoch:   0 loss: 3.611\n",
      "epoch: 100 loss: 3.384\n",
      "epoch: 200 loss: 2.764\n",
      "epoch: 300 loss: 2.329\n",
      "epoch: 400 loss: 2.066\n",
      "epoch: 500 loss: 1.890\n",
      "epoch: 600 loss: 1.748\n",
      "epoch: 700 loss: 1.629\n",
      "epoch: 800 loss: 1.525\n",
      "epoch: 900 loss: 1.429\n",
      "Training DCNN...\n",
      "epoch:   0 loss: 4.211\n",
      "epoch: 100 loss: 3.282\n",
      "epoch: 200 loss: 2.593\n",
      "epoch: 300 loss: 2.184\n",
      "epoch: 400 loss: 1.920\n",
      "epoch: 500 loss: 1.738\n",
      "epoch: 600 loss: 1.599\n",
      "epoch: 700 loss: 1.483\n",
      "epoch: 800 loss: 1.383\n",
      "epoch: 900 loss: 1.294\n",
      "Fold: 10, accuracy of CNN:  0.654\n",
      "Fold: 10, accuracy of DCNN: 0.718\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "np.float = float \n",
    "# If available, we use the GPU to train and evaluate the network\n",
    "# (If using Google Colab, make sure that you select a runtime with GPU in\n",
    "# the menu item \"Edit/notebook settings\" or \"Runtime/Change runtime type\".)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  print(\"Train on GPU\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"GPU is not available, we will use CPU instead of GPU\")\n",
    "\n",
    "# DTW computations will be executed on the CPU\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "accuracies_cnn = []\n",
    "accuracies_dcnn = []\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(data, labels):\n",
    "  fold = fold + 1\n",
    "\n",
    "  train_data = data[train_index]\n",
    "  train_labels = labels[train_index]\n",
    "  test_data = data[test_index]\n",
    "  test_labels = labels[test_index]\n",
    "\n",
    "  # Training of CNN. This is simultaneously the pre-train phase of DCNN.\n",
    "\n",
    "  train_dataset = torch.utils.data.TensorDataset(\n",
    "      torch.Tensor(train_data), \n",
    "      torch.LongTensor(train_labels) \n",
    "  )\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "      train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "  cnn = Net2_CNN()\n",
    "\n",
    "  #use:\n",
    "  #cnn = Net1_CNN() for experiment with Net1 from the manuscript\n",
    "  #cnn = Net2_CNN() for experiment with Net2 from the manuscript\n",
    "  \n",
    "  cnn.to(device)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(cnn.parameters(), lr=1e-5)\n",
    "\n",
    "  running_loss = 0.0\n",
    "  running_n = 0\n",
    "\n",
    "  print(\"Training CNN...\")\n",
    "\n",
    "  for epoch in range(1000):  \n",
    "    for inputs, targets in trainloader:\n",
    "      inputs = inputs.to(device) \n",
    "      targets = targets.to(device) \n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      outputs = cnn(inputs)\n",
    "\n",
    "      loss = criterion(outputs, targets)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "      running_n = running_n + 1\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"epoch: {:3d} loss: {:4.3f}\".format(epoch, running_loss/running_n))\n",
    "      running_loss = 0.0\n",
    "      running_n = 0\n",
    "\n",
    "  # Obtain the parameters of the dynamic convolutional layer, and \n",
    "  # precalculate its activations\n",
    "  params = []\n",
    "  for p in cnn.parameters():\n",
    "    params.append(p)\n",
    "\n",
    "  convolutional_filters = np.array(params[0].to(cpu_device).detach().numpy(), \n",
    "                                   dtype=np.float)\n",
    "\n",
    "  dc_activations_train = dc_activations(train_data, convolutional_filters)\n",
    "\n",
    "  # Train DCNN\n",
    "\n",
    "  train_dataset = torch.utils.data.TensorDataset(\n",
    "      torch.Tensor(dc_activations_train), \n",
    "      torch.LongTensor(train_labels) \n",
    "  )\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "      train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "\n",
    "  #use:  \n",
    "  #dcnn = Net1_DCNN() for experiments with Net1 from the manuscript\n",
    "  #dcnn = Net2_DCNN() for experiments with Net2 from the manuscript\n",
    "\n",
    "  dcnn = Net2_DCNN()\n",
    "\n",
    "  dcnn.to(device)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(dcnn.parameters(), lr=1e-5)\n",
    "\n",
    "  running_loss = 0.0\n",
    "  running_n = 0\n",
    "\n",
    "  print(\"Training DCNN...\")\n",
    "\n",
    "  for epoch in range(1000):  \n",
    "    for inputs, targets in trainloader:\n",
    "      inputs = inputs.to(device) \n",
    "      targets = targets.to(device) \n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      outputs = dcnn(inputs)\n",
    "\n",
    "      loss = criterion(outputs, targets)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "      running_n = running_n + 1\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"epoch: {:3d} loss: {:4.3f}\".format(epoch, running_loss/running_n))\n",
    "      running_loss = 0.0\n",
    "      running_n = 0\n",
    "  \n",
    "  acc, _, _ = eval_net(cnn, test_data, device)\n",
    "  accuracies_cnn.append(acc)\n",
    "  print(\"Fold: {:2d}, accuracy of CNN:  {:4.3f}\".format(fold, acc))\n",
    "\n",
    "  dc_activations_test = dc_activations(test_data, convolutional_filters)\n",
    "  acc, _, _ = eval_net(dcnn, dc_activations_test, device)\n",
    "  print(\"Fold: {:2d}, accuracy of DCNN: {:4.3f}\".format(fold, acc))\n",
    "  accuracies_dcnn.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2336518-2f07-409e-98f5-db6f4ad750f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Adiac**\n",
      "\n",
      "\n",
      "\t\tp-value:        0.001\n",
      "\t\tMean acc. CNN:  0.571\n",
      "\t\tMean acc. DCNN: 0.653\n",
      "\t\tStd. acc. CNN:  0.059\n",
      "\t\tStd. acc. DCNN: 0.062\n"
     ]
    }
   ],
   "source": [
    "print(\"**{}**\\n\\n\".format(file_name_prefix.split('/')[-1]))\n",
    "print(\"\\t\\tp-value:        {:4.3f}\".format(scipy.stats.ttest_rel(accuracies_cnn, accuracies_dcnn)[1]))\n",
    "print(\"\\t\\tMean acc. CNN:  {:4.3f}\".format(np.mean(accuracies_cnn)))\n",
    "print(\"\\t\\tMean acc. DCNN: {:4.3f}\".format(np.mean(accuracies_dcnn)))\n",
    "print(\"\\t\\tStd. acc. CNN:  {:4.3f}\".format(np.std(accuracies_cnn)))\n",
    "print(\"\\t\\tStd. acc. DCNN: {:4.3f}\".format(np.std(accuracies_dcnn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc7063-e0c7-4a04-980c-78819ccdeea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
